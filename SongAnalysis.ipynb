{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-09-30T09:20:00.910590Z",
          "start_time": "2024-09-30T09:20:00.894459Z"
        },
        "id": "initial_id",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-mwd1WyXr-Vn",
      "metadata": {
        "id": "-mwd1WyXr-Vn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "root_dir = r'speech-emotion-recognition-ravdess-data'\n",
        "\n",
        "data = []\n",
        "\n",
        "emotion_dict = {\n",
        "    '01': 'neutral',\n",
        "    '02': 'calm',\n",
        "    '03': 'happy',\n",
        "    '04': 'sad',\n",
        "    '05': 'angry',\n",
        "    '06': 'fearful',\n",
        "    '07': 'disgust',\n",
        "    '08': 'surprised'\n",
        "}\n",
        "\n",
        "for subdir, _, files in os.walk(root_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.wav'):\n",
        "            file_path = os.path.join(subdir, file)\n",
        "\n",
        "            identifiers = file.split('-')\n",
        "\n",
        "            emotion_code = identifiers[2]  # 3rd part of the filename\n",
        "            emotion_label = emotion_dict.get(emotion_code, 'unknown')\n",
        "\n",
        "            actor_id_with_ext = identifiers[6]  # 7th part of the filename (string with extension)\n",
        "            actor_id = actor_id_with_ext.split('.')[0]  # Remove the '.wav' extension\n",
        "            gender = 'male' if int(actor_id) % 2 != 0 else 'female'  # Check odd/even for gender\n",
        "\n",
        "            y, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "            chromagram = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "            mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "            spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "            tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
        "\n",
        "            mfccs_mean = np.mean(mfccs, axis=1)\n",
        "            chromagram_mean = np.mean(chromagram, axis=1)\n",
        "            mel_spectrogram_mean = np.mean(mel_spectrogram, axis=1)\n",
        "            spectral_contrast_mean = np.mean(spectral_contrast, axis=1)\n",
        "            tonnetz_mean = np.mean(tonnetz, axis=1)\n",
        "\n",
        "            feature_vector = np.concatenate((mfccs_mean, chromagram_mean, mel_spectrogram_mean, spectral_contrast_mean, tonnetz_mean))\n",
        "\n",
        "            data.append([file_path, *feature_vector, emotion_label, gender])\n",
        "\n",
        "columns = ['file_path'] + [f'feature_{i}' for i in range(len(feature_vector))] + ['label', 'gender']\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "df.to_csv('extracted_features_with_gender.csv', index=False)\n",
        "\n",
        "print(\"Feature extraction completed and saved to 'extracted_features_with_gender.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef7cef9b6fca7f11",
      "metadata": {
        "id": "ef7cef9b6fca7f11"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('extracted_features_with_gender.csv')\n",
        "\n",
        "emotion_counts = df['label'].value_counts()\n",
        "print(\"Sample counts for each emotion class:\")\n",
        "print(emotion_counts)\n",
        "\n",
        "min_samples = emotion_counts.min()\n",
        "print(f\"\\nMinimum samples in the dataset: {min_samples}\")\n",
        "\n",
        "balanced_data = []\n",
        "for emotion in emotion_counts.index:\n",
        "    emotion_subset = df[df['label'] == emotion]\n",
        "\n",
        "    # If the emotion class has fewer samples than the minimum, oversample\n",
        "    if len(emotion_subset) < min_samples:\n",
        "        # Oversampling: Randomly sample with replacement\n",
        "        oversampled_subset = emotion_subset.sample(min_samples, replace=True)\n",
        "        balanced_data.append(oversampled_subset)\n",
        "    else:\n",
        "        # Undersampling: Randomly sample without replacement\n",
        "        undersampled_subset = emotion_subset.sample(min_samples)\n",
        "        balanced_data.append(undersampled_subset)\n",
        "\n",
        "balanced_df = pd.concat(balanced_data, ignore_index=True)\n",
        "\n",
        "balanced_df.to_csv('balanced_extracted_features.csv', index=False)\n",
        "print(\"\\nDataset balanced and saved to 'balanced_extracted_features.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c26864f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "balanced_df = pd.read_csv('balanced_extracted_features.csv')\n",
        "\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "feature_indices = {\n",
        "    'MFCC': range(1, 14),         # MFCC features are from index 1 to 13 (13 features)\n",
        "    'Chroma': range(14, 26),      # Chroma features are from index 14 to 25 (12 features)\n",
        "    'Mel-Spectrogram': range(26, 126),  # Mel features are from index 26 to 125 (100 features)\n",
        "    'Spectral Contrast': range(126, 133),  # Spectral contrast features are from index 126 to 132 (7 features)\n",
        "    'Tonnetz': range(133, 139)    # Tonnetz features are from index 133 to 138 (6 features)\n",
        "}\n",
        "\n",
        "def plot_feature_distribution(data, feature_indices, feature_name):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.boxplot(data=data.iloc[:, feature_indices], palette='Set2')\n",
        "    plt.title(f'Distribution of {feature_name} Features')\n",
        "    plt.xlabel('Features')\n",
        "    plt.ylabel('Values')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize each set of features\n",
        "for feature_name, indices in feature_indices.items():\n",
        "    plot_feature_distribution(balanced_df, indices, feature_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac49b3d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Load the balanced dataset\n",
        "balanced_df = pd.read_csv('balanced_extracted_features.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "features = balanced_df.iloc[:, 1:-2]  # All feature columns (assuming the last two are label and gender)\n",
        "labels = balanced_df['label']  # Emotion labels\n",
        "\n",
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode the labels\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Standardize the features\n",
        "standardized_features = scaler.fit_transform(features)\n",
        "\n",
        "# Create a new DataFrame with standardized features and encoded labels\n",
        "processed_df = pd.DataFrame(standardized_features, columns=features.columns)\n",
        "processed_df['encoded_label'] = encoded_labels\n",
        "\n",
        "# Optionally, you can also include gender if needed\n",
        "# processed_df['gender'] = balanced_df['gender'].values\n",
        "\n",
        "# Save the processed data to a new CSV file\n",
        "processed_df.to_csv('processed_features_with_encoded_labels.csv', index=False)\n",
        "\n",
        "print(\"Label encoding and feature standardization completed and saved to 'processed_features_with_encoded_labels.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82e2b62e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Load the processed dataset\n",
        "processed_df = pd.read_csv('processed_features_with_encoded_labels.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "X = processed_df.drop(columns=['encoded_label'])  # Features\n",
        "y = processed_df['encoded_label']  # Encoded emotion labels\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_dist = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],  # Different layer configurations\n",
        "    'activation': ['logistic', 'tanh', 'relu'],                   # Activation functions\n",
        "    'solver': ['sgd', 'adam'],                                   # Optimization algorithms\n",
        "    'alpha': [0.0001, 0.001, 0.01],                              # L2 regularization term\n",
        "    'learning_rate': ['constant', 'invscaling', 'adaptive'],     # Learning rate strategies\n",
        "    'max_iter': [200, 500, 1000]                                 # Number of iterations\n",
        "}\n",
        "\n",
        "# Set up the RandomizedSearchCV\n",
        "mlp_random_search = RandomizedSearchCV(estimator=MLPClassifier(random_state=42),\n",
        "                                       param_distributions=param_dist,\n",
        "                                       n_iter=50,                       # Number of random searches\n",
        "                                       cv=3,                            # Number of cross-validation folds\n",
        "                                       verbose=1,                       # Verbosity level\n",
        "                                       random_state=42,                 # For reproducibility\n",
        "                                       n_jobs=-1)                       # Use all available cores\n",
        "\n",
        "# Fit the model to the training data\n",
        "dt_random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_dt_model = dt_random_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_dt = best_dt_model.predict(X_test)\n",
        "\n",
        "# Evaluate the best model\n",
        "dt_conf_matrix = confusion_matrix(y_test, y_pred_dt)\n",
        "dt_class_report = classification_report(y_test, y_pred_dt)\n",
        "\n",
        "# Display the results\n",
        "print(\"Best Parameters for Decision Tree:\\n\", dt_random_search.best_params_)\n",
        "print(\"Confusion Matrix for Decision Tree:\\n\", dt_conf_matrix)\n",
        "print(\"\\nClassification Report for Decision Tree:\\n\", dt_class_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e4bced7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from scipy.stats import randint\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the processed dataset\n",
        "processed_df = pd.read_csv('processed_features_with_encoded_labels.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "X = processed_df.drop(columns=['encoded_label'])  # Features\n",
        "y = processed_df['encoded_label']  # Encoded emotion labels\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Define parameter grid for SVM\n",
        "param_dist = {\n",
        "    \"C\": [0.1, 1, 10, 100],  # Regularization parameter\n",
        "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],  # Kernel types\n",
        "    \"gamma\": [\"scale\", \"auto\"],  # Kernel coefficient\n",
        "    \"degree\": randint(2, 5),  # Degree for 'poly' kernel\n",
        "}\n",
        "\n",
        "# Set up RandomizedSearchCV for SVM\n",
        "svm_random_search = RandomizedSearchCV(\n",
        "    estimator=SVC(random_state=42),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "# Fit the model and find the best parameters\n",
        "svm_random_search.fit(X_train, y_train)\n",
        "best_svm_model = svm_random_search.best_estimator_\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred_svm = best_svm_model.predict(X_test)\n",
        "svm_conf_matrix = confusion_matrix(y_test, y_pred_svm)\n",
        "svm_class_report = classification_report(y_test, y_pred_svm)\n",
        "\n",
        "# Display results\n",
        "print(\"Best Parameters for SVM:\\n\", svm_random_search.best_params_)\n",
        "print(\"Confusion Matrix for SVM:\\n\", svm_conf_matrix)\n",
        "print(\"\\nClassification Report for SVM:\\n\", svm_class_report)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
